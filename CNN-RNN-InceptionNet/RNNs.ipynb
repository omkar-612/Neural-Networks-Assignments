{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8e21a252c524ccf8eed0a47855b4767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_559a76aeaaff4b9e8d074e4c8ab69e3b",
              "IPY_MODEL_cdd6a8ec109341d8ab79c126c5fb57fc",
              "IPY_MODEL_acba40bff7144d84a41cb45f8d575c40"
            ],
            "layout": "IPY_MODEL_c21096674e8e4a848280956f2c5c6f03"
          }
        },
        "559a76aeaaff4b9e8d074e4c8ab69e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed55c41214904a9d9f4447a93b8b1913",
            "placeholder": "​",
            "style": "IPY_MODEL_c0cbc4b8cd8d4b3498348187e87e1c40",
            "value": "Reading GloVe Embeddings: 100%"
          }
        },
        "cdd6a8ec109341d8ab79c126c5fb57fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b006c2ee8c914c3090e8ee9e0516487b",
            "max": 400000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b73056e5b314979a7cf126a05d59f8e",
            "value": 400000
          }
        },
        "acba40bff7144d84a41cb45f8d575c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ab865f219f474dae1ddf24aa616c61",
            "placeholder": "​",
            "style": "IPY_MODEL_563cbb0180564e8ead8e6eb6c6636743",
            "value": " 400000/400000 [00:10&lt;00:00, 42442.60it/s]"
          }
        },
        "c21096674e8e4a848280956f2c5c6f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed55c41214904a9d9f4447a93b8b1913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0cbc4b8cd8d4b3498348187e87e1c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b006c2ee8c914c3090e8ee9e0516487b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b73056e5b314979a7cf126a05d59f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8ab865f219f474dae1ddf24aa616c61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563cbb0180564e8ead8e6eb6c6636743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d23dce847dc49a0bb4710588178cdcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa7936d2225e465da67a1ef83fde3736",
              "IPY_MODEL_010104df671447b3b9f7c10a16bfabdc",
              "IPY_MODEL_a2051b61a5ef4b9bb3ba1e10687dcafb"
            ],
            "layout": "IPY_MODEL_fdd62ef596884be3a2d243c01385d2ea"
          }
        },
        "aa7936d2225e465da67a1ef83fde3736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_261c9062bcb04f26aa1463f8089dbaf1",
            "placeholder": "​",
            "style": "IPY_MODEL_fbacbfe01a984d13a70d2b7ed23c3cf2",
            "value": "Forward Pass: 100%"
          }
        },
        "010104df671447b3b9f7c10a16bfabdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61de574d812948bf9dee46fd7ace4202",
            "max": 36,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ca4f20bc2a046c2aaeecc0659637b2d",
            "value": 36
          }
        },
        "a2051b61a5ef4b9bb3ba1e10687dcafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75739adc615c4b3dafc3e36a891a0066",
            "placeholder": "​",
            "style": "IPY_MODEL_2fcf9fa4af10463e8f7ba2e56b778656",
            "value": " 36/36 [00:00&lt;00:00, 138.80it/s]"
          }
        },
        "fdd62ef596884be3a2d243c01385d2ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "261c9062bcb04f26aa1463f8089dbaf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbacbfe01a984d13a70d2b7ed23c3cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61de574d812948bf9dee46fd7ace4202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ca4f20bc2a046c2aaeecc0659637b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75739adc615c4b3dafc3e36a891a0066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fcf9fa4af10463e8f7ba2e56b778656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "IF-dv2i-E2rS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the second NNFL assignment. In this assignment you will be programming an RNN from scratch and creating a preproccessing pipeline for Natural Language Processing. While RNNs are typically programmed using frameworks like PyTorch, the preprocessing pipeline that you will learn about here will be applicable in a lot of NLP problems you will face.\n",
        "\n",
        "Please read the instructions given below carefully before attempting the assignment.  \n",
        "- Do NOT import any other modules\n",
        "- Do NOT change the prototypes of any of the functions\n",
        "- Sample test cases are already given, test your code using these sample cases\n",
        "- Grading will be based on hidden test cases\n",
        "- Please solve this notebook using [Google Colab](https://colab.research.google.com/) as the required packages are already installed "
      ],
      "metadata": {
        "id": "oU7CZ9X2xcJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE`, as well as your name and ID number below:"
      ],
      "metadata": {
        "id": "YMonCIu5vq9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NAME = \"OMKAR R DURGADA\"\n",
        "ID = \"2020A3PS0460P\" "
      ],
      "metadata": {
        "id": "4QXpkM3O8IhT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the Dataset and the GloVe embeddings"
      ],
      "metadata": {
        "id": "WDjBIyzkwT8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will kick things off by installing all the pretrained models and the dataset. Running the below cell should set you up.\n",
        "\n",
        "While glove embeddings would have been covered in class, you can find some links about them below:\n",
        "\n",
        "1. [Glove paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "2. [For the lazy ones](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)"
      ],
      "metadata": {
        "id": "8K2YTcH_v6Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "! unzip glove.6B.zip\n",
        "! rm glove.6B.100d.txt glove.6B.200d.txt glove.6B.300d.txt glove.6B.zip\n",
        "! pip install --upgrade --no-cache-dir gdown\n",
        "! gdown --id 1iczQ7UlHsARuHuV8z3yhD-ByC-jwejI0"
      ],
      "metadata": {
        "id": "1PZMWiSep0aN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88960d78-ad99-4ad4-d7d0-381144265c81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-13 06:28:18--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-01-13 06:28:18--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.11MB/s    in 2m 39s  \n",
            "\n",
            "2023-01-13 06:30:58 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.0\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iczQ7UlHsARuHuV8z3yhD-ByC-jwejI0\n",
            "To: /content/RNN_From_Scratch_Dataset.txt\n",
            "100% 141k/141k [00:00<00:00, 54.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the requisite libraries. Keep in mind the ones that are imported - you will be needing them at a later point."
      ],
      "metadata": {
        "id": "scO0QLY4xUBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BSyzuofb1PXX"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "GpP01giz873E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d41f744-6b65-4dc9-e58f-ce6a081d72e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement"
      ],
      "metadata": {
        "id": "grOOyzwW1URn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem we will try to solve is next word prediction. Given a sequence of words, we want to train an RNN cell to predict the next most probable word."
      ],
      "metadata": {
        "id": "XKCDcr5C1VuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initialises all your parameters in the model. Each of these will be explained in due course of time."
      ],
      "metadata": {
        "id": "IVp2QxBdA8Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 8\n",
        "VOCAB_SIZE = 2000\n",
        "EMB_DIM = 50\n",
        "HIDDEN_LEN = 64\n",
        "BATCH_SIZE = 64\n",
        "DATA_PATH = '/content/RNN_From_Scratch_Dataset.txt'"
      ],
      "metadata": {
        "id": "tWUhSvFAA54Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing [3.25M]"
      ],
      "metadata": {
        "id": "U43E0qrJ1RT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing of data involves the following steps for each line of your dataset:\n",
        "1. Remove all punctuation from the data. This is done so that our model does not encounter characters which will not contribute to the prediction of the next word.\n",
        "2. Convert the data into tokens - in this case the tokens would just be words. You should have a clear understanding of the difference between words and tokens. You can read about other types of tokenisers [here](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
        "3. Removing the Stop Words. Stop words are available in abundance in any human language. A stop word is a commonly used word (such as “the”, “a”, “an”, “in”). By removing these words, we remove the low-level information from our text in order to give more focus to the important information. You can read more about them [here](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html).\n",
        "4. Pad the token sequence with padding tokens, or slice it depending on its length. This is done so that all your datapoints are of the same length. This allows us to ensure that when pytorch creates a batch, no errors are encountered. The inspiration for padding, however, comes from transformers. You can read more about them [here](https://arxiv.org/abs/1706.03762)."
      ],
      "metadata": {
        "id": "FQ4rQ8Ka0Frb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first write functions for each of these individual tasks. Note that each of these will be for a single datapoint."
      ],
      "metadata": {
        "id": "N2gG_oCt1s8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.5 Marks\n",
        "def clean_str_and_tokenise(line):\n",
        "\t'''\n",
        "\t\tSTEP 1:\n",
        "\t\t\tRemove punctuation marks from the input string and convert the entire string to lowercase\n",
        "\t\t\tchars_to_remove = [',', '.', '\"', \"'\", '/', '*', ',', '?', '!', '-', '\\n', '“', '”', '_', '&', '\\ufeff', '&', ';', \":\"]\n",
        "\t\tSTEP 2:\n",
        "\t\t\tTokenize (convert the clean string into a list with each word being a separate element)\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tline: The raw text string\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tlist of words in lowercase without punctuations\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\tchars_to_remove = [',', '.', '\"', \"'\", '/', '*', ',', '?', '!', '-', '\\n', '“', '”', '_', '&', '\\ufeff', '&', ';', \":\"]\n",
        "\tres=\"\"\n",
        "\t# Removing punctuations in string\n",
        "\t# Using loop + punctuation string\n",
        "\tfor ele in line:\n",
        "\t\tif ele not in chars_to_remove:\n",
        "\t\t\tres+=ele\n",
        "\n",
        "\tline = res.lower()\n",
        "\tline = line.split(\" \")\n",
        " \n",
        "\treturn line\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "aR16p2Fn1rki"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "test_str = 'Never, GONNA* give. you- up?'\n",
        "assert clean_str_and_tokenise(test_str) == ['never', 'gonna', 'give', 'you', 'up']\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "d9NLS0BW_5vZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bafd4d6-7ede-4d75-d8ff-b4fa0ed3ebd5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Test passed 👍\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.5 Marks\n",
        "def remove_stop_words(list_of_words):\n",
        "    '''\n",
        "    NOTE: \n",
        "    use nltk.corpus.stopwords.words('english') to get the list of stop words that should not be included in the final list\n",
        "\n",
        "    arguments:\n",
        "        list_of_words: a list of lowercased words\n",
        "\n",
        "    Returns:\n",
        "        list of words without stopwords\n",
        "\n",
        "\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "    filtered_sentence = [w for w in list_of_words if not w.lower() in stop_words]\n",
        "\n",
        "    return filtered_sentence\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "Pqh0Yowh7IT6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "test_list = ['what', 'does', 'the', 'fox', 'say']\n",
        "remove_stop_words(test_list)"
      ],
      "metadata": {
        "id": "JsqwQT3P7IQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2afda8-78cc-4bf8-e607-fcf5391f398f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fox', 'say']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: We do not always remove the stop words. The removal of stop words is highly dependent on the task we are performing and the goal we want to achieve. For example, if we are training a model that can perform the sentiment analysis task, we might not remove the stop words. \n",
        "\n",
        "As you can see here, a sentence, which is an important question: \"What Does the Fox Say?\" has now been converted into a phrase \"fox say\", removing the crux of the sentence. The model can no longer wonder what the fox says\n",
        "\n",
        "*SO WE WILL NOT REMOVE THE STOP WORDS ANYMORE*\n",
        "\n",
        "------------"
      ],
      "metadata": {
        "id": "VEIUv1jQ_bLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All our training points should have the same length. This is done to ensure that the pytorch dataloader can use them with minimal effort from our side.\n",
        "\n",
        "Another reason for this is that RNNs operate sequentially. Having, say, 200 tokens per training point(which might just be true for our dataset) will cause the training process to slow down. Moreover, RNNs struggle with long sequences.\n",
        "\n",
        "**Note:** Since this function works with preprocessed raw strings, we can also use it to create our labels. Therefore, before slicing/padding, ensure that you have extracted the label and have updated the training datapoint accordingly."
      ],
      "metadata": {
        "id": "6FU_fMOs35hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def pad_sequence(tokens, seq_len, padding_token = '<PAD>'):\n",
        "\t'''\n",
        "\tPadding/slicing sequences of tokens to ensure all of them have the same length. \n",
        "\tAfter the padding is done, the next word label is also appended to it.\n",
        "\n",
        "\tArguments:\n",
        "\ttokens: tokens generated from the tokenizer\n",
        "\tseq_len: The maximum permitted length of the sequence\n",
        "\tpadding_token: The token to be used in case of padding\n",
        "\n",
        "\tReturns:\n",
        "\ttokens: A list of padded/sliced tokens of len = max_len\n",
        "\tlast_token: The label for one datapoint, the token with the highest index\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\n",
        "\tif len(tokens) > seq_len:\n",
        "\t\tlast_token = tokens[seq_len]\n",
        "\n",
        "\telse:\n",
        "\t\tlast_token = tokens.pop()\n",
        "\n",
        "\tfor i in range(seq_len):\n",
        "\t\ttokens.append(padding_token)\n",
        "\n",
        "\treturn tokens[:seq_len], last_token\n",
        "\n",
        "\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "O8wbDshU3lgl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "test_seq1 = ['Who', 'let', 'the', 'dogs', 'out']\n",
        "test_seq2 = ['Testing','for','token','sequence','equal','to','sequence','length']\n",
        "test_seq3 = ['Testing','for','token','sequence','which','is','greater','than','sequence','length']\n",
        "tokens1, last_token1 = pad_sequence(test_seq1, SEQ_LEN)\n",
        "tokens2, last_token2 = pad_sequence(test_seq2, SEQ_LEN)\n",
        "tokens3, last_token3 = pad_sequence(test_seq3, SEQ_LEN)\n",
        "print(last_token3)\n",
        "assert tokens1 == ['Who', 'let', 'the', 'dogs', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] and last_token1 == 'out'\n",
        "assert tokens2 == ['Testing','for','token','sequence','equal','to','sequence','<PAD>'] and last_token2 == 'length'\n",
        "assert tokens3 == ['Testing','for','token','sequence','which','is','greater','than'] and last_token3 == 'sequence'\n",
        "print('Sample Tests passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "7oJQaQAuA3VU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1e35c3-fa47-4522-b026-dcb77c7c38cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence\n",
            "Sample Tests passed 👍\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the above functions should now be called from one function, which will preprocess the entire dataset."
      ],
      "metadata": {
        "id": "5GeIBn8X3s7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 1.25 Marks\n",
        "def preprocess_data(path, seq_len):\n",
        "\t'''\n",
        "\t\tFunction to call all preprocessing steps except removing stop words for the entire corpus: i.e: remove the punctuation, tokenize and pad it\n",
        "\n",
        "\t\tNote: Ensure to leave a slot in the vocabulary for the <UNK> token\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tpath: The path to your data file\n",
        "\t\t\tseq_len: The maximum permitted length of the token sequences\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tdatapoints: The preprocessed training + testing points for the corpus, list format\n",
        "      labels: The labels to be used per datapoint, list format\n",
        "\t'''\n",
        "\n",
        "\t# YOUR CODE HERE\n",
        "\t# Using readlines()\n",
        "\tfile1 = open(path, 'r')\n",
        "\tlines = file1.readlines()\n",
        "\tdatapoints, labels = [], []\n",
        "\tfor line in lines:\n",
        "\t\ttokens = clean_str_and_tokenise(line)\n",
        "\t\tpadded_tokens, label = pad_sequence(tokens, seq_len)\n",
        "\t\tdatapoints.append(padded_tokens)\n",
        "\t\tlabels.append(label)\n",
        "  \n",
        "\treturn datapoints, labels\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "1zNt-wkF0FBQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to preprocess your data."
      ],
      "metadata": {
        "id": "vbdWybl2BQya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapoints, labels = preprocess_data(DATA_PATH, SEQ_LEN)"
      ],
      "metadata": {
        "id": "YP52HLLGBOx-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "assert len(datapoints[0]) == len(datapoints[-1]) == len(datapoints[1231]) == SEQ_LEN\n",
        "assert len(labels) == len(datapoints)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "-xD5qxXYjv0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2e5f7c-ea6e-4805-d6a3-3854424893ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Test passed 👍\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the vocab and converting the tokens to numbers [1.25M]"
      ],
      "metadata": {
        "id": "sXoz96tj6ACJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More often than not, you will have a lot of words in your dataset - more than you require. Therefore, VOCAB_SIZE becomes a parameter that you can manually set as per your requirements.\n",
        "\n",
        "Your model cannot work with textual words - it needs numbers. For this purpose, we convert the words into numbers by creating a one-one mapping between words and a set of indices.\n",
        "\n",
        "For creating the vocabulary, we will be choosing the top-k words(k = user-defined vocabulary size) in our dataset. We also need a way to work with words not in our vocab - thus comes the ```<UNK>``` token. Any word not in our vocabulary is allotted this token. This has to manually be added into the dataset.\n",
        "\n",
        "We will also create an inverse mapping which can be used for decoding the next word from our model."
      ],
      "metadata": {
        "id": "VO8Q7Cvo6FYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(datapoints, labels, vocab_size):\n",
        "\t'''\n",
        "\t\tBuilding the vocabulary from the most common words in the corpus\n",
        "\n",
        "\t\tNote: Ensure to leave a slot in the vocabulary for the <UNK> token. \n",
        "\t\tFor uniformity, insert this at the end of your vocab, ie, its index should be vocab_size-1.\n",
        "\t\tAlso ensure that each label is in the vocab. If not, add it by removing the least common word.\n",
        "\t\tAlso ensure you remove padding tokens from the vocabulary and add the most appropriate word.\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tdatapoints: The preprocessed datapoints in the corpus\n",
        "\t\t\tlabels: The labels per datapoint in the corpus\n",
        "\t\t\tvocab_size: The number of tokens in the vocab\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tvocab: A dicitionary mapping from the word to its corresponding vocab index(0 indexed)\n",
        "\t\t\tvocab_inv: A dicitionary mapping from the vocab index to its corresponding word\n",
        "\t'''\t\n",
        "\n",
        "\tword_sea = []\n",
        "\tfor datapoint in datapoints:\n",
        "\t\tword_sea.extend(datapoint)\n",
        "\tfor datapoint in labels:\n",
        "\t\tword_sea.append(datapoint)\n",
        "\t\n",
        "\tmost_common_words = [word for word, _ in Counter(word_sea).most_common(vocab_size - 1)]\n",
        "\treplaced_idx = 1\n",
        "\tfor label in labels:\n",
        "\t\tif label not in most_common_words:\n",
        "\t\t\twhile(most_common_words[-replaced_idx] in labels):\n",
        "\t\t\t\treplaced_idx += 1\n",
        "\t\t\tmost_common_words[-replaced_idx] = label\n",
        "\t\n",
        "\tvocab = {word: idx for idx, word in enumerate(most_common_words)}\n",
        "\tvocab_inv = {idx: word for idx, word in enumerate(most_common_words)}\n",
        "\tvocab['<UNK>'] = vocab_size - 1\n",
        "\tvocab_inv[vocab_size - 1] = '<UNK>'\n",
        "\treturn vocab, vocab_inv"
      ],
      "metadata": {
        "id": "7tHkfb2O1Wzq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will convert the tokens in our dataset to tokens in our created vocabulary. This means that tokens not in our vocabulary wil get mapped to ```<UNK>```"
      ],
      "metadata": {
        "id": "WISklrPR7PiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data2tokens(vocab, raw_data):\n",
        "\t'''\n",
        "\t\tConverts the raw text into their corresponding tokens\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tvocab: Mapping from the word to its corresponding vocab index\n",
        "\t\t\traw_data: The preprocessed data, however, some words are not present as \n",
        "\t\t\t\t\t\t\t  tokens in the vocab\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tdataset_tokens: A list of the preprocessed data where all words are correspoding to \n",
        "\t\t\t\t\t\t\t\t\t    tokens in the vocab\n",
        "\t'''\n",
        "\tdataset_tokens = []\n",
        "\tfor data in raw_data:\n",
        "\t\tdataset_tokens.append([word if word in vocab else '<UNK>' for word in data])\n",
        "\t\n",
        "\treturn dataset_tokens"
      ],
      "metadata": {
        "id": "CNwRF3Y27PAk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above tokens can now mapped to indices in our vocab."
      ],
      "metadata": {
        "id": "hFMDZMWm7t1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 1.25 Marks\n",
        "def tokens2ids(vocab, data_tokens):\n",
        "\t'''\n",
        "\t\tConverts the tokens into their corresponding vocab indices\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tvocab: Mapping from the word to its corresponding vocab index\n",
        "\t\t\tdata_tokens: The preprocessed data where all words are correspoding to \n",
        "\t\t\t\t\t\t\t\t\t tokens in the vocab\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tdataset_ids: The tokens in the dataset converted to their vocab indices\n",
        "\t\t\tThis should be a Pytorch Long Tensor\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\tdataset_ids = []\n",
        "\tfor data in data_tokens:\n",
        "\t\tdataset_ids.append([vocab[word] for word in data])\n",
        "  \n",
        "\treturn torch.LongTensor(dataset_ids)\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "CE2MWnYF2WzG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to call all the functions above in sequence."
      ],
      "metadata": {
        "id": "VAjmy8498B3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, vocab_inv = build_vocab(datapoints, labels, VOCAB_SIZE)\n",
        "dataset_tokens = data2tokens(vocab, datapoints)\n",
        "dataset_ids = tokens2ids(vocab, dataset_tokens)"
      ],
      "metadata": {
        "id": "ADQMNCBX8BWU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "assert len(vocab) == VOCAB_SIZE\n",
        "assert len(dataset_tokens) == len(dataset_ids)\n",
        "assert torch.is_tensor(dataset_ids)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "pcfdgSEai10g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fa1ad0-0f9d-4fd8-d0b0-047d2cbf3cea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Test passed 👍\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is ready, we need to find a way to feed it into the model. As you have learnt in the previous assignment, we use mini batch sampling for inputs into the model. PyTorch uses a dataloader class(which you used in your previous assignment) which makes this possible. This next function will be an emulation of the dataloader in vanilla Python."
      ],
      "metadata": {
        "id": "FJKBUpzy8Jcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(datapoints, labels, num_batches, batch_size):\n",
        "  '''\n",
        "    Function to create the dataloader which will yield batches on the fly.\n",
        "\n",
        "    Arguments:\n",
        "      datapoints: The preprocessed datapoints in the corpus\n",
        "      labels: The labels per datapoint\n",
        "      num_batches: The number of batches from the dataset\n",
        "      batch_size: The number of datapoints per batch\n",
        "    Returns:\n",
        "      x: One minibatch of input indices of size (batch_size, seq_len)\n",
        "      y: One minibatch of labels per datapoints of size (batch_size, 1)\n",
        "  '''\n",
        "  n = len(datapoints)\n",
        "  for i in range(num_batches):\n",
        "      if (i+1)*batch_size < n:\n",
        "          x = datapoints[i*batch_size: (i+1)*batch_size]\n",
        "          y = labels[i*batch_size: (i+1)*batch_size]\n",
        "      else:\n",
        "          x = datapoints[i*batch_size:]\n",
        "          y = labels[i*batch_size:]\n",
        "      yield x, y"
      ],
      "metadata": {
        "id": "jrLu-dxj717y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0E05lbcW8aKw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling [5.5M]"
      ],
      "metadata": {
        "id": "WfEKhLJI8xBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step would be to build an RNN cell. **This weights would be randomly initialized from a Normal distribution, and initialize the biases to all zeros**. An RNN contains 5 matrices, each of which have been described in the docstring. You would return a List with the values in the following order: $[I, H, O, I_b, O_b]$. \n",
        "\n",
        "The equations of an RNN can be summarised as:\n",
        "\n",
        "### $ h^{(t)} = tanh(E \\cdot I) + h^{(t - 1)} \\cdot H + I_b $\n",
        "###  $ o^{(t)} = h^{(t - 1)} \\cdot O + O_b$"
      ],
      "metadata": {
        "id": "g5anN1Pr86a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def create_rnn(hidden_len, emb_dim):\n",
        "\t'''\n",
        "\t\tCreates a randomly intialised rnn cell\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\thidden_len: The length of the hidden state of the rnn\n",
        "\t\t\temb_dim: The length of the embeddings in the embedding space\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\ta list of:\n",
        "\t\t\t\trnn_i: The learnable weights to convert the input embeddings to the current hidden state\n",
        "\t\t\t\trnn_h: The learnable weights to convert the previous hidden state to the current hidden state\n",
        "\t\t\t\trnn_O: The learnable weights to convert the current hidden state to the output vector\n",
        "\t\t\t\trnn_i_bias: The bias to be used to convert the input embeddings to the current hidden state\n",
        "\t\t\t\trnn_O_bias: The bias to be used to convert the current hidden state to the output vector\n",
        "\n",
        "\t\t\tNOTE:\n",
        "\t\t\t Don't make the biases one dimentional, set the second dimention of the hidden layer to be 1\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\trnn_i = torch.randn(emb_dim, hidden_len)\n",
        "\trnn_h = torch.randn(hidden_len, hidden_len)\n",
        "\trnn_O = torch.randn(hidden_len,hidden_len)\n",
        "\trnn_i_bias = torch.zeros(hidden_len,1)\n",
        "\trnn_O_bias = torch.zeros(hidden_len,1)\n",
        " \n",
        "\treturn [rnn_i, rnn_h, rnn_O, rnn_i_bias, rnn_O_bias]\n",
        "\traise NotImplementedError\n"
      ],
      "metadata": {
        "id": "p27i3TB61mUA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model input, as discussed before, is going to be a set of indices corresponding to each token in the vocabulary. This cannot be directly fed in because they do not mean anything to our model. They are not present in a common vector space. For this purpose, we create \"embeddings\" which is a multi-dimensional representation of our vocabulary. These are stored in a lookup table and are learnable features, just like the weights and biases of our network. They can be indexed using the indices we have created in our vocab.\n",
        "\n",
        "\n",
        "You have to write a function to initialise this lookup table, as per the conditions given in the docstring. The ```load_pretrained_embeddings``` loads the GloVe Embeddings for you in a dictionary mapping the GloVe tokens to GloVe embeddings."
      ],
      "metadata": {
        "id": "3PrP3gNR9fHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained_embeddings(model_name):\n",
        "  '''\n",
        "\t\tReads and loads the pretrained glove embeddings from the downloaded glove file\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tmodel_name: The path to the pretrained glove file\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tembedding_model: Mapping from token to its correpsonding glove embedding\n",
        "\t'''\n",
        "  embedding_model = {}\n",
        "  f = open(model_name, 'r')\n",
        "  for line in tqdm(f.readlines(), desc = 'Reading GloVe Embeddings'):\n",
        "    tmp = line.split(' ')\n",
        "    word, vec = tmp[0], list(map(float, tmp[1:]))\n",
        "    assert(len(vec) == 50)\n",
        "    if word not in embedding_model:\n",
        "        embedding_model[word] = torch.tensor(vec)\n",
        "        \n",
        "  return embedding_model"
      ],
      "metadata": {
        "id": "h7OgAc_qVMgx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 1.5 Marks\n",
        "def create_embeddings(emb_dim, num_tokens, vocab, model_name = 'glove.6B.50d.txt'):\n",
        "  '''\n",
        "\t\tCreates and initialises the embeddings for the corpus:\n",
        "    1. If a token in the corpus is present as a token in the glove embeddings, initialise it with the glove embedding\n",
        "    2. If a token in the corpus is not present as a token in the glove embeddings, initialise it with a random embedding sampled from U(-0.25, 0.25)\n",
        "    3. Initialise the padding token with a zero embedding\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\temb_dim: The length of the embeddings in the embedding space\n",
        "      num_tokens: The number of tokens in the vocabulary, aka, vocab size\n",
        "      vocab: Mapping from the word to its corresponding vocab index\n",
        "      model_name: The path to the pretrained glove file\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tembeddings: The initialised embedding space (a torch tensor of shape (num_tokens, emb_dim))\n",
        "\t'''\n",
        "  # YOUR CODE HERE\n",
        "  glove_embeddings = load_pretrained_embeddings(model_name)\n",
        "\n",
        "  embeddings = torch.zeros(num_tokens, emb_dim)\n",
        "  \n",
        "  for key, value in vocab.items():\n",
        "    if key in glove_embeddings.keys():\n",
        "      embeddings[value] = glove_embeddings[key]\n",
        "    elif key == '<PAD>':\n",
        "      embeddings[value] = torch.zeros(emb_dim)\n",
        "    else:\n",
        "      embeddings[value] = torch.rand(emb_dim)*0.5 - 0.25\n",
        "    \n",
        "  return embeddings\n",
        "  raise NotImplementedError"
      ],
      "metadata": {
        "id": "j-6VnNoO1c4q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function creates your classifier, which is a fully connected layer. As before, you have to return a list which contains the weights and baises of the classifier. As before, sample the weights from a normal distribution and set the biases as zero. The equations of the classifier can be summarised as:\n",
        "\n",
        " ### $ Y = (X \\cdot W + b)$"
      ],
      "metadata": {
        "id": "TTGHzEc0-5cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def create_classifier(in_features, num_classes):\n",
        "\t'''\n",
        "\t\tCreates a randomly intialised classifer as a fully connected layer\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tin_features: The length of the feature vector at the input of the classifier\n",
        "\t\t\tnum_classes: The number of classes to be predicted\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t[weight, bias]\n",
        "\t\t\t\tweight: The randomly initialised weights for the fully connected layer from in_features to num_classes (use torch.randn)\n",
        "\t\t\t\tbias: The zero initialised bias for the fully connected layer from in_features to num_classes\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\tweight = torch.randn(in_features, num_classes)\n",
        "\tbias =  torch.zeros(num_classes)\n",
        "\treturn [weight, bias]\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "7nzd7LvA1VJ3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function forwards the rnn by one step for all elements in the batch. In case a padding token is encountered, no change is made to the output and the hidden state. For this purpose, you have been provided the previous hidden state and the previous output as an input into the function.\n",
        "\n",
        "### $ h^{(t)} = tanh(E \\cdot I) + h^{(t - 1)} \\cdot H + I_b $\n",
        "###  $ o^{(t)} = h^{(t - 1)} \\cdot O + O_b$"
      ],
      "metadata": {
        "id": "u4mHywfl_aH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def forward_rnn_one_step(rnn_parameters, embs, prev_hidden, prev_output):\n",
        "\t'''\n",
        "\t\tTakes one forward step through the rnn cell. In case a padding token is encountered, no change is made to the output and the hidden state.\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\trnn_parameters: List containing the weights and biases of the rnn\n",
        "\t\t\tembs: The input embeddings of the tokens of the datapoint\n",
        "\t\t\tprev_hidden: The previous hidden state\n",
        "\t\t\tprev_output: The previous outputs\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\thidden_state: The next hidden state\n",
        "\t\t\toutput: The output for this sequence\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\t[rnn_i, rnn_h, rnn_O, rnn_i_bias, rnn_O_bias] = rnn_parameters\n",
        "\t\n",
        "\thidden_state = torch.tanh(torch.matmul(embs,rnn_i) + torch.matmul(prev_hidden,rnn_h) + rnn_i_bias.T)\n",
        "\toutput = F.softmax(torch.matmul(prev_hidden, rnn_O) + rnn_O_bias.T)\n",
        "\n",
        "\treturn hidden_state, output\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "5GQRtbVU1ep6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function passes your features through the full connected layer. The equations are summarised again for your convenience:\n",
        "\n",
        " $ Y = (X \\cdot W + b)$"
      ],
      "metadata": {
        "id": "2NFRrShZ_0lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def classify(feat, classifier_parameters):\n",
        "\t'''\n",
        "\t\tPerforms a forward pass through the classifier\n",
        "\n",
        "\t\tArguments:\n",
        "\t\t\tfeat: The feature vector for classification\n",
        "\t\t\tclassifier_parameters: List containing the weights and biases of the classifier\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tlogits: The logits for each word in our model\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\t[weight, bias] = classifier_parameters\n",
        "\tlogits = torch.matmul(feat, weight) + bias\n",
        "\treturn logits\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "zAKtvg8V1g7P"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward function passes your input data through all the above functions in sequence. Note the sequential nature of calling the rnn, since the previous hidden state has to be used.\n",
        "\n",
        "The initial hidden state has to be initialised to zeros, while the output has to be randomly initialised.\n",
        "\n",
        "The features to be used for the classifier is the final output of the RNN. Note that the features can be a concatenations of all outputs/hidden states too. You will have to change the classifier accordingly. \n",
        "\n",
        "The output logits will have to be converted to a probability distribution. For this purpose, it will be passed through a softmax activation."
      ],
      "metadata": {
        "id": "SxbdiDe0AFPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 MARKS\n",
        "def forward(x, seq_len, hidden_len, classifier, embs, rnn_parameters):\n",
        "\t'''\n",
        "\t\tPerforms a foraward pass for the batched data\n",
        "\n",
        "\t\tArguments:\n",
        "\t\t\tx: The feature vector for classification\n",
        "\t\t\tseq_len: The maximum permitted length of the token sequences in the datapoints\n",
        "\t\t\thidden_len: The length of the hidden state of the rnn cell\n",
        "\t\t\tclassifier: List containing the weights and biases of the classifier\n",
        "\t\t\tembs: The input embeddings of the tokens of the datapoint\n",
        "\t\t\trnn_parameters: List containing the weights and biases of the rnn\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tprobs: The probabilities of all the next words\n",
        "\t'''\n",
        "\tinput_embs = embs[x]\n",
        "\thidden_state = torch.zeros(x.shape[0], hidden_len)\n",
        "\toutput = torch.zeros(x.shape[0], hidden_len)\n",
        "\tfor i in range(seq_len):\n",
        "\t\thidden_state, output = forward_rnn_one_step(rnn_parameters, input_embs[:, i, :], hidden_state, output)\n",
        "\t\t\n",
        "\tlogits = classify(output, classifier)\n",
        "\tprobs = F.softmax(logits, dim = 1)\n",
        "\treturn probs\n",
        "\traise NotImplementedError"
      ],
      "metadata": {
        "id": "7x9Pma-P1j1J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to call your modelling functions."
      ],
      "metadata": {
        "id": "Q28VzfXeBbNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(69)\n",
        "print('Creating RNN')\n",
        "[rnn_i, rnn_h, rnn_O, rnn_i_bias, rnn_O_bias] = create_rnn(HIDDEN_LEN, EMB_DIM)\n",
        "print('Creating Embeddings')\n",
        "embs = create_embeddings(EMB_DIM, VOCAB_SIZE, vocab)\n",
        "print('Creating Classifier')\n",
        "classifier = create_classifier(HIDDEN_LEN, VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "fNN-R2zRXNBC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "c8e21a252c524ccf8eed0a47855b4767",
            "559a76aeaaff4b9e8d074e4c8ab69e3b",
            "cdd6a8ec109341d8ab79c126c5fb57fc",
            "acba40bff7144d84a41cb45f8d575c40",
            "c21096674e8e4a848280956f2c5c6f03",
            "ed55c41214904a9d9f4447a93b8b1913",
            "c0cbc4b8cd8d4b3498348187e87e1c40",
            "b006c2ee8c914c3090e8ee9e0516487b",
            "3b73056e5b314979a7cf126a05d59f8e",
            "d8ab865f219f474dae1ddf24aa616c61",
            "563cbb0180564e8ead8e6eb6c6636743"
          ]
        },
        "outputId": "7fcd31d8-03f7-498f-9c82-df27272b2408"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RNN\n",
            "Creating Embeddings\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reading GloVe Embeddings:   0%|          | 0/400000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8e21a252c524ccf8eed0a47855b4767"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Classifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Cases\n",
        "assert rnn_h.shape == (HIDDEN_LEN, HIDDEN_LEN)\n",
        "assert all(x[0]==0 for x in rnn_i_bias)\n",
        "assert rnn_i_bias.shape == (64, 1)\n",
        "assert torch.isclose(rnn_i[40][40], torch.tensor(0.9098), atol = 0.1) \n",
        "assert embs.shape == (VOCAB_SIZE, EMB_DIM)\n",
        "assert len(classifier[1]) == VOCAB_SIZE\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "9oyK7TFckq5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2508f66-1270-4193-9ae1-2b8aee07432e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Test passed 👍\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Cases\n",
        "torch.manual_seed(69)\n",
        "rnn_parameters = create_rnn(HIDDEN_LEN, EMB_DIM)\n",
        "test_output, _ = forward_rnn_one_step(rnn_parameters, torch.randn(32, EMB_DIM), torch.zeros(32, HIDDEN_LEN), torch.zeros(32, HIDDEN_LEN))\n",
        "assert torch.isclose(test_output[1][0], torch.tensor(0.9), atol = 0.1)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "NAFTEyePFCYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657979ed-30c1-4dda-b490-497a7a924ab1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Test passed 👍\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-b5cac09181cf>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.softmax(torch.matmul(prev_hidden, rnn_O) + rnn_O_bias.T)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your data and model is ready, it is time to pass it through the model and get some predictions. Write an expression to calculate the number of batches and use that to create your dataloader, using the variables and prototype you have created above."
      ],
      "metadata": {
        "id": "3pd4vVHDBvR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.5 Marks\n",
        "# YOUR CODE HERE\n",
        "num_batches = int(len(labels)/BATCH_SIZE) # YOUR CODE HERE\n",
        "dataloader = create_dataloader(dataset_ids, labels, num_batches, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "CInFhfcsBOdk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like you do in PyTorch, loop through your dataloader to get the batches. Perform a forward pass to get the probabilities of your next words. Choose the most probable word using ```torch.argmax``` and add these to a list called ```preds```.\n",
        "\n",
        "These predictions will be indices, therefore, use ```vocab_inv``` to convert it back into words."
      ],
      "metadata": {
        "id": "GQIsUl-8B9wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for x, y in tqdm(dataloader, total = num_batches, desc = 'Forward Pass'):\n",
        "  probs = forward(x, SEQ_LEN, HIDDEN_LEN, classifier, embs, rnn_parameters)\n",
        "  next_word = torch.argmax(probs, dim = 1)\n",
        "  preds.extend([vocab_inv[int(word)] for word in next_word])"
      ],
      "metadata": {
        "id": "ZokoQcq-p8gZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9d23dce847dc49a0bb4710588178cdcd",
            "aa7936d2225e465da67a1ef83fde3736",
            "010104df671447b3b9f7c10a16bfabdc",
            "a2051b61a5ef4b9bb3ba1e10687dcafb",
            "fdd62ef596884be3a2d243c01385d2ea",
            "261c9062bcb04f26aa1463f8089dbaf1",
            "fbacbfe01a984d13a70d2b7ed23c3cf2",
            "61de574d812948bf9dee46fd7ace4202",
            "6ca4f20bc2a046c2aaeecc0659637b2d",
            "75739adc615c4b3dafc3e36a891a0066",
            "2fcf9fa4af10463e8f7ba2e56b778656"
          ]
        },
        "outputId": "61d798ec-aed9-428a-e71b-2a53c6b2939e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Forward Pass:   0%|          | 0/36 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d23dce847dc49a0bb4710588178cdcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-b5cac09181cf>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.softmax(torch.matmul(prev_hidden, rnn_O) + rnn_O_bias.T)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "assert len(preds) == len(datapoints)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "YfbJPR2mhN2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "322d6135-ea09-4182-c891-e490d462650f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3dc5e9e6584c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sample Test Case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample Test passed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\U0001F44D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preds), len(datapoints))"
      ],
      "metadata": {
        "id": "fHZN-2mFDAx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b35519-25be-4e96-f6aa-745bf5f7d533"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2304 2362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ETYZ2G1L6k_"
      },
      "source": [
        "# End of this part.\n",
        "Assignment by:\n",
        "\n",
        "Vighnesh Natarajan Ganesh (f20190131@pilani.bits-pilani.ac.in)\n",
        "\n",
        "Arnav Ahuja (f20180619@pilani.bits-pilani.ac.in)\n",
        "\n",
        "Devaansh Gupta (f20190187@pilani.bits-pilani.ac.in)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3gHw1qVK_I3"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}